{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60cb75e-2f75-412d-81b7-f99eb27b01e0",
   "metadata": {},
   "source": [
    "# Пошаговое руководство по созданию жидкой нейронной сети LTC с нуля\n",
    "Павел Наказненко, 2024\n",
    "\n",
    "[Liquid Time-Constant Networks on Arxiv](https://arxiv.org/abs/2006.04439)\n",
    "\n",
    "Благодарность: Это руководство в значительной степени основано на [реализации LTCCell](https://github.com/mlech26l/ncps/blob/master/ncps/torch/ltc_cell.py), спасибо авторам LNN.\n",
    "\n",
    "[Канал Telegram: ToShoSeti](https://t.me/toshoseti)\n",
    "\n",
    "## Отказ от ответственности\n",
    "Это руководство является попыткой ознакомиться с LNN и понять, как они работают. Это не идеальная реализация.\n",
    "Для меня это сложная тема, поэтому возможны ошибки и недопонимания. Пожалуйста, будьте внимательны."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a788f-ca24-4f2b-9df0-28eea3d87d11",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b6116-3c0a-4635-95c4-128b0815747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a634a2-a50d-49a3-ac1a-8582c093c08b",
   "metadata": {},
   "source": [
    "## Реализация класса RandomWiring\n",
    "Класс RandomWiring отвечает за определение архитектуры соединений между нейронами. Он инициализирует случайные матрицы смежности для соединений нейронов и сенсорных входов. Они используются в качестве произвольной разреженной матрицы позже в LIFNeuralLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa25083-2ede-4ae6-9524-92244b0f872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWiring:\n",
    "    def __init__(self, input_dim, output_dim, neuron_count):\n",
    "        self.input_dim = input_dim  # Количество входных признаков\n",
    "        self.output_dim = output_dim  # Количество выходных признаков\n",
    "        self.neuron_count = neuron_count  # Количество нейронов в слое\n",
    "        self.adjacency_matrix = np.random.uniform(0, 1, (neuron_count, neuron_count))  # Матрица смежности для соединений между нейронами\n",
    "        self.sensory_adjacency_matrix = np.random.uniform(0, 1, (input_dim, neuron_count))  # Матрица смежности для сенсорных входов к нейронам\n",
    "\n",
    "    def erev_initializer(self):\n",
    "        return np.random.uniform(-0.2, 0.2, (self.neuron_count, self.neuron_count))  # Инициализация потенциалов разворота для соединений нейронов\n",
    "\n",
    "    def sensory_erev_initializer(self):\n",
    "        return np.random.uniform(-0.2, 0.2, (self.input_dim, self.neuron_count))  # Инициализация потенциалов разворота для сенсорных входов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd71ad-1c3e-4b90-ac96-8835bcba9a9a",
   "metadata": {},
   "source": [
    "## Реализация класса LIFNeuronLayer\n",
    "Класс LIFNeuronLayer моделирует поведение слоя нейронов типа Leaky Integrate-and-Fire. Он инициализирует параметры нейронов и определяет прямой проход для вычисления состояний нейронов. Динамика LIF описывается с помощью ОДУ (обыкновенных дифференциальных уравнений), и во время прямого прохода мы вычисляем состояния, используя метод Эйлера Explicit, описанный в оригинальной статье."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7938b8-4841-4b11-8072-3bd3bf593baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeuronLayer(nn.Module):\n",
    "    def __init__(self, wiring, ode_unfolds=12, epsilon=1e-8):\n",
    "        super(LIFNeuronLayer, self).__init__()\n",
    "        self.wiring = wiring  # Объект Wiring, содержащий информацию о соединениях\n",
    "        self.ode_unfolds = ode_unfolds  # Количество итераций решателя ОДУ\n",
    "        self.epsilon = epsilon  # Малое значение, чтобы избежать деления на ноль\n",
    "        self.softplus = nn.Softplus()  # Функция активации Softplus\n",
    "\n",
    "        # Диапазоны инициализации для параметров\n",
    "        GLEAK_MIN, GLEAK_MAX = 0.001, 1.0\n",
    "        VLEAK_MIN, VLEAK_MAX = -0.2, 0.2\n",
    "        CM_MIN, CM_MAX = 0.4, 0.6\n",
    "        W_MIN, W_MAX = 0.001, 1.0\n",
    "        SIGMA_MIN, SIGMA_MAX = 3, 8\n",
    "        MU_MIN, MU_MAX = 0.3, 0.8\n",
    "        SENSORY_W_MIN, SENSORY_W_MAX = 0.001, 1.0\n",
    "        SENSORY_SIGMA_MIN, SENSORY_SIGMA_MAX = 3, 8\n",
    "        SENSORY_MU_MIN, SENSORY_MU_MAX = 0.3, 0.8\n",
    "\n",
    "        # Инициализация параметров нейронов случайными значениями в указанных диапазонах\n",
    "        self.gleak = nn.Parameter(torch.rand(wiring.neuron_count) * (GLEAK_MAX - GLEAK_MIN) + GLEAK_MIN)\n",
    "        self.vleak = nn.Parameter(torch.rand(wiring.neuron_count) * (VLEAK_MAX - VLEAK_MIN) + VLEAK_MIN)\n",
    "        self.cm = nn.Parameter(torch.rand(wiring.neuron_count) * (CM_MAX - CM_MIN) + CM_MIN)\n",
    "        self.w = nn.Parameter(torch.rand(wiring.neuron_count, wiring.neuron_count) * (W_MAX - W_MIN) + W_MIN)\n",
    "        self.sigma = nn.Parameter(torch.rand(wiring.neuron_count, wiring.neuron_count) * (SIGMA_MAX - SIGMA_MIN) + SIGMA_MIN)\n",
    "        self.mu = nn.Parameter(torch.rand(wiring.neuron_count, wiring.neuron_count) * (MU_MAX - MU_MIN) + MU_MIN)\n",
    "        self.erev = nn.Parameter(torch.Tensor(wiring.erev_initializer()))\n",
    "        \n",
    "        # Инициализация сенсорных параметров случайными значениями в указанных диапазонах\n",
    "        self.sensory_w = nn.Parameter(torch.rand(wiring.input_dim, wiring.neuron_count) * (SENSORY_W_MAX - SENSORY_W_MIN) + SENSORY_W_MIN)\n",
    "        self.sensory_sigma = nn.Parameter(torch.rand(wiring.input_dim, wiring.neuron_count) * (SENSORY_SIGMA_MAX - SENSORY_SIGMA_MIN) + SENSORY_SIGMA_MIN)\n",
    "        self.sensory_mu = nn.Parameter(torch.rand(wiring.input_dim, wiring.neuron_count) * (SENSORY_MU_MAX - SENSORY_MU_MIN) + SENSORY_MU_MIN)\n",
    "        self.sensory_erev = nn.Parameter(torch.Tensor(wiring.sensory_erev_initializer()))\n",
    "\n",
    "        # Маски разреженности (фиксированные, необучаемые) на основе матриц смежности соединений\n",
    "        self.sparsity_mask = torch.Tensor(np.abs(wiring.adjacency_matrix))\n",
    "        self.sensory_sparsity_mask = torch.Tensor(np.abs(wiring.sensory_adjacency_matrix))\n",
    "\n",
    "    def forward(self, inputs, state, elapsed_time=1.0):\n",
    "        return self.ode_solver(inputs, state, elapsed_time)\n",
    "\n",
    "    def ode_solver(self, inputs, state, elapsed_time):\n",
    "        v_pre = state  # Предыдущее состояние (напряжение)\n",
    "\n",
    "        # Предварительный расчет эффектов от сенсорных нейронов\n",
    "        sensory_activation = self.softplus(self.sensory_w) * self.sigmoid(inputs, self.sensory_mu, self.sensory_sigma)\n",
    "        sensory_activation = sensory_activation * self.sensory_sparsity_mask\n",
    "        sensory_reversal_activation = sensory_activation * self.sensory_erev\n",
    "\n",
    "        # Расчет числителя и знаменателя для сенсорных входов\n",
    "        w_numerator_sensory = torch.sum(sensory_reversal_activation, dim=1)\n",
    "        w_denominator_sensory = torch.sum(sensory_activation, dim=1)\n",
    "\n",
    "        # Расчет мембранной емкости во времени\n",
    "        cm_t = self.softplus(self.cm) / (elapsed_time / self.ode_unfolds)\n",
    "\n",
    "        # Инициализация весов для связей между нейронами\n",
    "        w_param = self.softplus(self.w)\n",
    "        for _ in range(self.ode_unfolds):\n",
    "            # Активация на основе предыдущего состояния\n",
    "            w_activation = w_param * self.sigmoid(v_pre, self.mu, self.sigma)\n",
    "            w_activation = w_activation * self.sparsity_mask\n",
    "            reversal_activation = w_activation * self.erev\n",
    "\n",
    "            # Расчет числителя и знаменателя для связей между нейронами\n",
    "            w_numerator = torch.sum(reversal_activation, dim=1) + w_numerator_sensory\n",
    "            w_denominator = torch.sum(w_activation, dim=1) + w_denominator_sensory\n",
    "\n",
    "            # Расчет проводимости утечки и напряжения\n",
    "            gleak = self.softplus(self.gleak)\n",
    "            numerator = cm_t * v_pre + gleak * self.vleak + w_numerator\n",
    "            denominator = cm_t + gleak + w_denominator\n",
    "\n",
    "            # Обновление состояния (напряжения)\n",
    "            v_pre = numerator / (denominator + self.epsilon)\n",
    "\n",
    "        return v_pre\n",
    "\n",
    "    def sigmoid(self, v_pre, mu, sigma):\n",
    "        v_pre = torch.unsqueeze(v_pre, -1)  # Расширение для соответствия размерам\n",
    "        activation = sigma * (v_pre - mu)  # Применение сигма и среднего смещения\n",
    "        return torch.sigmoid(activation)  # Применение сигмоидной активации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d292f2-b406-49e9-b857-c5e445abe57d",
   "metadata": {},
   "source": [
    "### Понимание нейронного слоя LIF\n",
    "\n",
    "Нейрон Leaky Integrate-and-Fire (LIF) — это простая и широко используемая модель в вычислительной нейробиологии. Он моделирует, как биологические нейроны интегрируют поступающие сигналы и срабатывают при достижении порога. Модель LIF отражает основные характеристики реальных нейронов и является эффективной в вычислительном плане.\n",
    "\n",
    "#### Математическая формулировка LIF-нейронов\n",
    "\n",
    "Нейрон LIF можно описать следующим дифференциальным уравнением:\n",
    "\n",
    "$$\\tau_m \\frac{dV(t)}{dt} = -V(t) + R_m I(t)$$\n",
    "\n",
    "Где:\n",
    "- $V(t)$ — мембранный потенциал в момент времени $t$.\n",
    "- $\\tau_m$ – постоянная времени мембраны.\n",
    "- $R_m$ – сопротивление мембраны.\n",
    "- $I(t)$ — входной ток в момент времени $t$.\n",
    "\n",
    "Когда мембранный потенциал $V(t)$ достигает определенного порога $V_{\\text{th}}$, нейрон запускает потенциал действия (или спайк), и $V(t)$ сбрасывается до более низкого значения, часто $V_{\\text{reset}}$.\n",
    "\n",
    "#### Реализация в классе LIFNeuronLayer\n",
    "\n",
    "Класс `LIFNeuronLayer` в нашей реализации имитирует слой LIF-нейронов с конкретными параметрами мембранного потенциала, проводимости и входных весов. \n",
    "##### Инициализация\n",
    "\n",
    "Класс инициализирует несколько параметров, управляющих поведением нейронов, в том числе:\n",
    "- **gleak**: проводимость утечки.\n",
    "- **vleak**: напряжение утечки.\n",
    "- **см**: емкость мембраны.\n",
    "- **w, sigma, mu**: параметры весов и функций активации.\n",
    "- **erev**: Реверсивные потенциалы нейронных связей.\n",
    "- **sensory_w, sensory_sigma, sensory_mu, sensory_erev**: Параметры сенсорных входов.\n",
    "\n",
    "#### Отмена активации\n",
    "\n",
    "- **Реверсивная активация**: представляет собой влияние синаптических реверсивных потенциалов на мембранный потенциал. В биологических нейронах реверсивный потенциал — это напряжение, при котором чистый поток определенного иона через мембрану равен нулю. Включение в модель реверсивных потенциалов помогает моделировать реалистичные синаптические взаимодействия.\n",
    "- **Расчет**: включает умножение активации на матрицу реверсивного потенциала, которая влияет на обновление состояния нейрона во время прямого прохода. Этот механизм помогает интегрировать эффекты возбуждающих и тормозных синапсов.\n",
    "\n",
    "##### Forward проход и солвер ОДУ\n",
    "\n",
    "Прямой проход «LIFNeuronLayer» включает решение дифференциального уравнения, которое управляет динамикой мембранного потенциала. Это делается с использованием итеративного подхода с несколькими развертываниями ОДУ.\n",
    "\n",
    "Упрощенно:\n",
    "1. **Предварительный расчет сенсорных эффектов**: рассчитываем сенсорную активацию и обратную активацию на основе входных данных.\n",
    "2. **Цикл солвера ОДУ**: итеративно обновляем состояния нейронов с помощью солвера ОДУ. Это включает в себя вычисление активаций, применение масок разреженности и обновление состояний напряжения.\n",
    "\n",
    "##### Сигмоидная функция\n",
    "\n",
    "Классическая сигмоидная функция масштабирует входные данные в диапазоне от 0 до 1, что важно для активации нейронов. Она используется как для нейронных связей, так и для сенсорных входов.\n",
    "Измененная сигмоидная функция в классе LIFNeuronLayer используется для включения параметров mu (среднее значение) и сигма (масштаб). Эта измененная реализация обеспечивает более гибкое и контролируемое поведение активации по сравнению с классическим torch.sigmoid. \n",
    "\n",
    "1. **Параметр «mu» (средний сдвиг)**:\n",
    "    - Параметр `mu` сдвигает входное напряжение (`v_pre`). Этот сдвиг позволяет центрировать функцию активации вокруг различных средних значений.\n",
    "    - Математически `v_pre - mu` сдвигает входные данные так, что средняя точка (где сигмоида равна 0,5) не обязательно равна нулю, а `mu`.\n",
    "\n",
    "2. **Параметр `sigma` (Масштаб)**:\n",
    "    - Параметр `sigma` масштабирует входное напряжение (`v_pre`). Это масштабирование контролирует крутизну сигмовидной функции.\n",
    "    - Математически `sigma * (v_pre - mu)` регулирует наклон сигмовидной кривой. Большая «сигма» делает сигмовидную круче, а меньшая «сигма» делает ее более плавной.\n",
    "\n",
    "Измененная сигмовидная функция используется для обеспечения большей гибкости в динамике активации нейронной сети:\n",
    "\n",
    "1. **Биологический реализм**:\n",
    "    - Биологические нейроны не имеют фиксированного порога активации или кривой ответа. Кривая отклика может смещаться и масштабироваться в зависимости от различных факторов.\n",
    "    - Объединив «мю» и «сигму», мы можем смоделировать эту изменчивость, сделав модель более биологически правдоподобной.\n",
    "\n",
    "2. **Улучшенное обучение**:\n",
    "    - Нейронные сети могут извлечь выгоду из функций адаптивной активации. Разным слоям или нейронам может потребоваться разное поведение активации для эффективного изучения сложных паттернов.\n",
    "    - Параметры «mu» и «sigma» можно подобрать во время обучения, что позволяет сети динамически настраивать свои функции активации.\n",
    "3. **Расширенный контроль**:\n",
    "    - В некоторых случаях контроль над средним значением и масштабом функции активации может улучшить способность сети обрабатывать различные входные диапазоны и распределения.\n",
    "    - Такая настройка может привести к лучшей сходимости и производительности в определенных задачах, особенно в тех, которые связаны с разнообразными и сложными входными сигналами.\n",
    "\n",
    "#### Зачем нужны маски случайной смежности и разреженности?\n",
    "\n",
    "- **Матрица случайной смежности**: эта матрица определяет связи между нейронами случайным образом, моделируя сложные и нерегулярные связи, обнаруженные в биологических нейронных сетях. Это вносит в сеть изменчивость и сложность, что может помочь в изучении более разнообразных моделей.\n",
    "- **Маска разреженности**: эта маска гарантирует, что активны только определенные соединения, обеспечивая разреженность сети. Разреженные соединения имитируют эффективную проводку мозга, где не каждый нейрон связан с каждым другим нейроном, что снижает вычислительную нагрузку и предотвращает переобучение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b07a6d2-1493-4540-927a-268978f1a196",
   "metadata": {},
   "source": [
    "## Реализация класса LTCCell\n",
    "Класс LTCCell представляет одну ячейку в Liquid Time-Constant Recurrent Neural Network. Он использует LIFNeuronLayer для обновления состояний нейронов.\n",
    "\n",
    "Примечание: мы предполагаем, что количество нейронов всегда больше выходного измерения, поэтому мы просто обрезаем их до выходных значений. Если это не так, можно использовать дополнительный полносвязный слой для проецирования скрытых состояний на выходные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c20767-040b-4848-8ecf-efe85c009908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTCCell(nn.Module):\n",
    "     def __init__(self, wiring, in_features=None, ode_unfolds=6, epsilon=1e-8):\n",
    "         super(LTCCell, self).__init__()\n",
    "         self.wiring = wiring # Объект Wiring\n",
    "         self.ode_unfolds = ode_unfolds # Количество итераций решателя ОДУ\n",
    "         self.epsilon = epsilon # Небольшое значение, чтобы избежать деления на ноль\n",
    "\n",
    "         self.neuron = LIFNeuronLayer(wiring, ode_unfolds, epsilon) # Инициализируем LIFNeuron с заданными соединениями\n",
    "\n",
    "     def forward(self, inputs,states, elapsed_time=1.0):\n",
    "         next_state = self.neuron(inputs,states,elapsed_time) # Вычисляем следующее состояние, используя модель нейрона\n",
    "         outputs = next_state[:, :self.wiring.output_dim] # Сопоставляем состояние с выходными измерениями\n",
    "         return outputs, next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcae71a-2f04-411d-8b41-483503761b01",
   "metadata": {},
   "source": [
    "## Реализация класса LTCRNN\n",
    "Класс LTCRNN создает рекуррентную нейронную сеть, используя несколько экземпляров LTCCell. Он обрабатывает последовательности входных данных для получения последовательностей выходных данных.\n",
    "\n",
    "Примечание: В этом руководстве предполагается, что LTCRNN будет вызываться один раз для всей последовательности, поэтому здесь происходит инициализация состояния нулевым значением при каждом вызове forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf2844-d177-4945-a456-c766c7eb93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTCRNN(nn.Module):\n",
    "    def __init__(self, wiring, input_dim, hidden_dim, output_dim):\n",
    "        super(LTCRNN, self).__init__()\n",
    "        self.cell = LTCCell(wiring, in_features=input_dim)  # Инициализация LTCCell с wiring и размером входных данных\n",
    "        self.hidden_dim = hidden_dim  # Количество скрытых нейронов\n",
    "        self.output_dim = output_dim  # Количество выходных нейронов\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, _ = inputs.size()  # Получение размера батча и длины последовательности из размеров входных данных\n",
    "        \n",
    "        states = torch.zeros(batch_size, self.hidden_dim)  # Инициализация скрытых состояний нулями\n",
    "            \n",
    "        outputs = []  # Список для хранения выходных данных для каждого временного шага\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            output, states = self.cell(inputs[:, t, :], states)  # Вычисление выходных данных и следующего состояния для каждого временного шага\n",
    "            outputs.append(output)  # Добавление выходных данных в список\n",
    "\n",
    "        result = torch.stack(outputs, dim=1)  # стэкаем выходные данные вдоль размерности последовательности\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d2fc0-2ad6-40a9-b5b3-d6ce9c8cfc0e",
   "metadata": {},
   "source": [
    "## Генерация спиральных данных\n",
    "Мы сгенерируем набор данных спиральных траекторий для обучения и оценки нашей модели. Функция generate_spiral_data создает синтетические точки данных, образующие спиральный узор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2f4b6-53f8-44ea-b4e8-16e132a19d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_data(num_points, num_turns, noise = 2):\n",
    "    theta = np.linspace(0, num_turns * 2 * np.pi, num_points)\n",
    "    z = np.linspace(0, 1, num_points)\n",
    "    r = z\n",
    "    x = r * np.sin(theta) + noise * np.random.randn(*theta.shape) / num_points\n",
    "    y = r * np.cos(theta) + noise * np.random.randn(*theta.shape) / num_points\n",
    "    return np.stack([x, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6eeb8-fece-479d-9a19-b247e7e68deb",
   "metadata": {},
   "source": [
    "## Обучение модели\n",
    "Далее мы устанавливаем гиперпараметры и готовим данные для обучения. Мы определяем цикл обучения для обучения модели с использованием сгенерированных спиральных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf55c7-e2b9-499b-b446-1478cc93b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "input_dim = 2 # Количество входных размерностей\n",
    "hidden_dim = 8 # Количество скрытых размерностей (количество нейронов в LIFNeuralLayer)\n",
    "output_dim = 2 # Количество выходных размерностей\n",
    "num_points = 500 # Количество точек спирали в наборе данных\n",
    "num_turns = 3 # Количество витков спирали\n",
    "learning_rate = 0.005\n",
    "num_epochs = 2000\n",
    "seq_len = 3 # Максимальная длина последовательности выборки\n",
    "batch_size = 32\n",
    "\n",
    "# Генерация данных\n",
    "data = generate_spiral_data(num_points, num_turns)\n",
    "all_inputs = data[:-1, :]\n",
    "all_targets = data[1:, :]\n",
    "\n",
    "# Подготовка входных и целевых последовательностей\n",
    "trajectory_count = max(1, len(all_inputs) - seq_len)\n",
    "train_inputs = [torch.FloatTensor(all_inputs[i:i + seq_len]) for i in range(trajectory_count)]\n",
    "train_targets = [torch.FloatTensor(all_targets[i:i + seq_len]) for i in range(trajectory_count)]\n",
    "\n",
    "# Перемешивание и разделение данных для обучения\n",
    "random_train_indices = np.arange(len(train_inputs))\n",
    "np.random.shuffle(random_train_indices)\n",
    "train_split_index = int(len(random_train_indices) * 0.8)\n",
    "random_train_indices = random_train_indices[:train_split_index]\n",
    "\n",
    "# Функция для создания батчей\n",
    "def create_batches(data_list, batch_size):\n",
    "    return [data_list[i:i + batch_size] for i in range(0, len(data_list), batch_size)]\n",
    "\n",
    "# Создание батчей входных и целевых данных\n",
    "train_input_batches = create_batches(train_inputs, batch_size)\n",
    "train_target_batches = create_batches(train_targets, batch_size)\n",
    "\n",
    "# Инициализация модели\n",
    "wiring = RandomWiring(input_dim, output_dim, hidden_dim)\n",
    "model = LTCRNN(wiring, input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Цикл обучения\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()    \n",
    "    total_loss = 0\n",
    "\n",
    "    # Итерация по партиям\n",
    "    for x, y_target in zip(train_input_batches, train_target_batches):\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.stack(x)  # Складывание партии последовательностей\n",
    "        y_target = torch.stack(y_target)  # Складывание партии целей\n",
    "        outputs = model(x)  # Прямой проход через модель\n",
    "        loss = criterion(outputs, y_target)  # Вычисление потерь\n",
    "\n",
    "        # Накопление общих потерь и выполнение обратного прохода и шага оптимизации\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Вывод потерь каждые 100 эпох и построение прогнозов\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Прогнозирование и построение графика\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(torch.FloatTensor(all_inputs).unsqueeze(0))\n",
    "            np_predictions = predictions.squeeze(0).numpy()\n",
    "            val_loss = criterion(predictions, torch.FloatTensor(all_targets).unsqueeze(0))  # Вычисление потерь\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Total train loss: {total_loss:.4f}, Total val loss: {val_loss:.4f}')\n",
    "\n",
    "        plt.plot(all_targets[:, 0], all_targets[:, 1], 'g-', label='Истинный путь')\n",
    "        plt.plot(np_predictions[:, 0], np_predictions[:, 1], 'r-', label='Прогнозируемый путь')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83619c-a4db-4c24-8848-82bcd300a71c",
   "metadata": {},
   "source": [
    "# Заключение\n",
    "В этом руководстве мы реализовали классический LTC LNN, как описано в оригинальной статье. \n",
    "\n",
    "В архитектуре LNN доступны также и другие улучшения:\n",
    "* [Closed-form Continuous-Time Network models](https://arxiv.org/abs/2106.13898)\n",
    "* [LTC-SE](https://arxiv.org/abs/2304.08691)\n",
    "* [Liquid-S4](https://arxiv.org/abs/2209.12951)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae93978-0b3d-4ab9-9a5c-6f6299e96576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
